Title: Machine Learning
Date: 2018-10-24 03:12
Author: pythonmarketer
Category: Uncategorized
Slug: 1518
Status: draft

**linear regression** - A type of [**regression model**](https://developers.google.com/machine-learning/glossary/#regression_model) that outputs a continuous value from a linear combination of input features.

**logistic regression** - A model that generates a probability for each possible discrete label value in classification problems by applying a [**sigmoid function**](https://developers.google.com/machine-learning/glossary/#sigmoid_function) to a linear prediction. Logistic regression valued more than linear regression in machine learning.

**sigmoid function** - A function that maps logistic or multinomial regression output (log odds) to probabilities, returning a value between 0 and 1.

A lot of the common loss functions, including the following, are convex functions:

-   L2 loss
-   Log Loss
-   L1 regularization
-   L2 regularization  (most common?)

The loss function for linear regression is squared loss. Squared Loss aka L2 loss. squared loss reacts more strongly to outliers than L1 loss.

The loss function for binary logistic regression is Log Loss.

**Loss** - "A measure of how far a model's predictions are from its label. Or, to phrase it more pessimistically, a measure of how bad the model is. To determine this value, a model must define a loss function. For example, linear regression models typically use mean squared error for a loss function, while logistic regression models use Log Loss." - Google Machine Learning Crash Course

gradient descent

**mean squared error**  
The average squared loss per example. MSE is calculated by dividing the squared loss by the number of examples. An example is one row of the dataset.

**binary classification** - A type of classification task that outputs one of two mutually exclusive classes.

**classification threshold** - In general, raising the classification threshold reduces false positives, thus raising precision.

**training** **dataset **- partition of data set aside for training model.

**validation** **dataset** - partition of data set aside data for iteratively evaluating model.

**test dataset** - partition of data set aside for final testing of model. Do not touch until the end.

**hyperparameters** - features used to tune the model

convert raw data --> feature vector --> strings mapped with one-hot encoding --> integers easily convert to floating point

features

-   good feature values used 5 or more times in dataset

 

> **do machine learning like the great engineer you are, not like the great machine learning expert you aren’t.**
>
> Most of the problems you will face are, in fact, engineering problems. Even with all the resources of a great machine learning expert, most of the gains come from great features, not great machine learning algorithms. So, the basic approach is:

1.  Make sure your pipeline is solid end to end.

2.  Start with a reasonable objective.

3.  Add common­-sense features in a simple way.

4.  Make sure that your pipeline stays solid.

Google's [Rules of Machine Learning](https://developers.google.com/machine-learning/guides/rules-of-ml/)

 

[Practical Advice for Analysis of Large Datasets](http://www.unofficialgoogledatascience.com/2016/10/practical-advice-for-analysis-of-large.html)
